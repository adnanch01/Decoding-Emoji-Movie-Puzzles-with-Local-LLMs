model,prompt_type,temperature,accuracy,mean_latency,p95_latency
llama3.2:latest,few_shot,0.0,26.7,0.43,0.62
llama3.2:latest,few_shot,0.3,33.3,0.43,0.55
llama3.2:latest,few_shot,0.7,26.7,0.43,0.59
llama3.2:latest,json_constrained,0.0,26.7,0.43,0.54
llama3.2:latest,json_constrained,0.3,33.3,0.42,0.53
llama3.2:latest,json_constrained,0.7,26.7,0.43,0.63
llama3.2:latest,zero_shot,0.0,60.0,0.85,2.83
llama3.2:latest,zero_shot,0.3,53.3,0.27,0.37
llama3.2:latest,zero_shot,0.7,46.7,0.27,0.36
mistral:7b,few_shot,0.0,26.7,1.86,5.41
mistral:7b,few_shot,0.3,26.7,1.79,4.76
mistral:7b,few_shot,0.7,20.0,2.16,5.33
mistral:7b,json_constrained,0.0,13.3,1.14,1.46
mistral:7b,json_constrained,0.3,13.3,1.43,2.66
mistral:7b,json_constrained,0.7,6.7,1.1,1.43
mistral:7b,zero_shot,0.0,0.0,1.42,4.59
mistral:7b,zero_shot,0.3,0.0,1.29,4.73
mistral:7b,zero_shot,0.7,6.7,1.68,5.17
